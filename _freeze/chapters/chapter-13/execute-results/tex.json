{
  "hash": "66b0e2957156ac97a249b9df0beeb0a3",
  "result": {
    "markdown": "# Continuous exposures\n\n## Calculating propensity scores for continuous exposures\n\nPropensity scores generalize to many other types of exposures, including continuous exposures.\nAt its heart, the workflow is the same: we fit a model where the exposure is the outcome then use that model to weight a second outcome model.\nFor continuous exposures, linear regression is the simplest way to create propensities.\nInstead of probabilities, we use the cumulative density function.\nThen, we use this density to weight the outcome model.\n\nLet's take a look at an example.\nIn the `tourplans` data set, we have information about the posted waiting times for rides.\nWe also have a limited amount of data on the observed, actual times.\nThe question that we will consider is this: Do posted wait times for the Seven Dwarves Mine Train at 8 am affect actual wait times at 9 am?\nHere's our DAG:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Proposed DAG for the relationship between posted wait in the morning at a particular park and the average wait time between 5pm and 6pm.](chapter-13_files/figure-pdf/dag-avg-wait-1.pdf)\n:::\n:::\n\n\n\nWe're assuming that our primary confounders are the time the park closes, historic high temperatures, whether or not the ride has extra magic morning hours, and the ticket season.\nThe confounders precede the exposure and outcome, and (by definition) the exposure precedes the outcome.\nThe average posted wait time is, in theory, a manipulable exposure because the park could post a time different from what they expect.\n\nThe model is similar to the binary exposure case, but we need to use linear regression, as the posted time is a continuous variable.\nSince we're not using probabilities, we'll instead calculate denominators for our weights from a normal density.\nWe then calculate the denominator using the `dnorm()` function, which calculates the normal density for the `exposure`, using `.fitted` as the mean and `mean(.sigma)` as the SD.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(\n  exposure ~ confounder_1 + confounder_2,\n  data = df\n) |>\n  augment(data = df) |>\n  mutate(\n    denominator = dnorm(exposure, .fitted, mean(.sigma, na.rm = TRUE))\n  )\n```\n:::\n\n\n\n## Diagnostics and stabilization\n\nContinuous exposure weights, however, are very sensitive to modeling choices.\nOne problem particular is the existence of extreme weights, an issue that can also affect other types of exposures.\nWhen some observations have extreme weights, the propensities are *destabilized,* which result in wider confidence intervals.\nWe can stabilize them using the marginal distribution of the exposure.\nA common way to calculate the marginal distribution for propensity scores is to use a regression model with no predictors.\n\n::: def-box\nExtreme weights destabilize estimates, resulting in wider confidence intervals.\nExtreme weights can be an issue for any time of weight (including those for binary and other types of exposures) that is not bounded.\nBounded weights like the ATO (which are bounded to 0 and 1) do not have this problem, however--one of their many benefits.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for continuous exposures\nlm(\n  exposure ~ 1,\n  data = df\n) |>\n  augment(data = df) |>\n  transmute(\n    numerator = dnorm(exposure, .fitted, mean(.sigma, na.rm = TRUE))\n  )\n\n# for binary exposures\nglm(\n  exposure ~ 1,\n  data = df,\n  family = binomial()\n) |>\n  augment(type.predict = \"response\", data = df) |>\n  select(numerator = .fitted)\n```\n:::\n\n\n\nThen, rather than inverting them, we calculate the weights as `numerator / denominator`.\nLet's try it out on our posted wait times example.\nFirst, let's wrangle our data to address our question: do posted wait times at 8 affect actual weight times at 9?\nWe'll join the baseline data (all covariates and posted wait time at 8) with the outcome (average actual time).\nWe also have a lot of missingness for `avg_sactmin`, so we'll drop unobserved values for now.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(touringplans)\neight <- seven_dwarfs_train_2018 |>\n  filter(hour == 8) |>\n  select(-avg_sactmin)\n\nnine <- seven_dwarfs_train_2018 |>\n  filter(hour == 9) |>\n  select(date, avg_sactmin)\n\nwait_times <- eight |>\n  left_join(nine, by = \"date\") |>\n  drop_na(avg_sactmin)\n```\n:::\n\n\n\nFirst, let's calculate our denominator model.\nWe'll fit a model using `lm()` for `avg_spostmin` with our covariates, then use the fitted predictions of `avg_spostmin` (`.fitted`) to calculate the density using `dnorm()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\ndenominator_model <- lm(\n  avg_spostmin ~\n    close + extra_magic_morning + weather_wdwhigh + wdw_ticket_season,\n  data = wait_times\n)\n\ndenominators <- denominator_model |>\n  augment(data = wait_times) |>\n  mutate(\n    denominator = dnorm(\n      avg_spostmin,\n      .fitted,\n      mean(.sigma, na.rm = TRUE)\n    )\n  ) |>\n  select(date, denominator)\n```\n:::\n\n\n\nWhen we just use the inverted values of `denominator`, we end up with several extreme weights:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndenominators |>\n  mutate(wts = 1 / denominator) |>\n  ggplot(aes(wts)) +\n  geom_density(col = \"#E69F00\", fill = \"#E69F0095\") +\n  scale_x_log10() +\n  theme_minimal(base_size = 20) +\n  xlab(\"Weights\")\n```\n\n::: {.cell-output-display}\n![](chapter-13_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nLet's now fit the marginal density to use for stabilized weights:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnumerator_model <- lm(\n  avg_spostmin ~ 1,\n  data = wait_times\n)\n\nnumerators <- numerator_model |>\n  augment(data = wait_times) |>\n  mutate(\n    numerator = dnorm(\n      avg_spostmin,\n      .fitted,\n      mean(.sigma, na.rm = TRUE)\n    )\n  ) |>\n  select(date, numerator)\n```\n:::\n\n\n\nWe also need to join the fitted values back to our original data set by date, then calculate the stabilized weights (`swts`) using `numerator / denominator`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwait_times_wts <- wait_times |>\n  left_join(numerators, by = \"date\") |>\n  left_join(denominators, by = \"date\") |>\n  mutate(swts = numerator / denominator)\n```\n:::\n\n\n\nThe stabilized weights are much less extreme.\nStabilized weights should have a mean close to 1 (in this example, it is `round(mean(wait_times_wts$swts), digits = 2)`); when that is the case, then the psuedo-population (that is, the equivalent number of observations after weighting) is equal to the original sample size.\nIf the mean is far from 1, we may have issues with model mispecification or violations of positivity [@hernÃ¡n2021].\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(wait_times_wts, aes(swts)) +\n  geom_density(col = \"#E69F00\", fill = \"#E69F0095\") +\n  scale_x_log10() +\n  theme_minimal(base_size = 20) +\n  xlab(\"Stabilized Weights\")\n```\n\n::: {.cell-output-display}\n![](chapter-13_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWhen we compare the exposure--average posted wait times--to the standardized weights, we still have one particularly high weight.\nIs this a problem, or is this a valid data point?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(wait_times_wts, aes(avg_spostmin, swts)) +\n  geom_point() +\n  geom_point(\n    data = function(x) filter(x, swts > 10),\n    color = \"firebrick\"\n  ) +\n  geom_text(\n    data = function(x) filter(x, swts > 10),\n    aes(label = date),\n    hjust = 0,\n    nudge_x = -14,\n    color = \"firebrick\"\n  ) +\n  theme_minimal(base_size = 20) +\n  scale_y_log10() +\n  labs(x = \"Average Posted Wait\", y = \"Stabilized Weights\")\n```\n\n::: {.cell-output-display}\n![](chapter-13_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nIt turns out that on June 23, 2018, the ride had an [interactive queue](https://disneyparks.disney.go.com/blog/2018/06/disney-doodle-pluto-sniffs-out-fun-at-seven-dwarfs-mine-train/).\nOur model predicted a much lower wait time, so this date was upweighted.\n\n## Fitting the outcome model for continuous exposures\n",
    "supporting": [
      "chapter-13_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}