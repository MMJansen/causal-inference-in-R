\mainmatter

# What is a causal question? {#sec-causal-question}

{{< include 00-setup.qmd >}}

## Schrödinger's Causality

The heart of causal analysis is the causal question; it dictates what data we analyze, how we analyze it, and to which populations we make inferences about.
This book, being applied in nature, deals primarily with the analysis stage of causal inference.
Relative to the complexity of specifying a good causal question, the analysis stage is considerably more straightforward.
In the first six chapters of this book, we'll discuss what a causal question is, how to improve our questions, and consider some examples.

Causal questions are part of a broader set of questions we can ask with statistical techniques related to the primary tasks of data science: description, prediction, and causal inference [@hernán2019].
Unfortunately, these tasks are often muddled by both the techniques we use (regression, for instance, is helpful for all three tasks) and the way we talk about them.
When researchers are interested in causal inference from non-randomized data, we often use euphemistic language like "association" instead of declaring our intent to estimate a causal effect [@Hernán2018].

In a recent study of the language of analyses in epidemiologic research, for instance, the most common root word describing the estimated effect was "associate," but many researchers also felt that "associate" implied at least *some* causal effect (@fig-word-ranking) [@haber_causal_language].
Only around 1% of the studies analyzed used the root word "cause" at all.
Yet, a third of studies had action recommendations, and researchers rated 80% of these recommendations as having at least some causal implication.
Often, these studies have stronger action recommendations (alluding to causal effects) than those implied by the description of the effect (root words like "associate" and "compare").
Despite how many studies implied that the goal was causal inference, only about 4% used formal causal models like those discussed in this book.
However, most discussed how such a cause might be justified by previous research or theory.

```{r}
#| label: "fig-word-ranking"
#| fig.cap: "Rankings of causal strength of root words used by researchers. Root words with more Strong rankings have stronger causal implications than those with many None or Weak rankings. Data from Haber et al."
#| fig.height: 9
#| echo: false

rankings <-  read_csv(here::here("data/word_rankings.csv"), show_col_types = FALSE) |> 
  janitor::clean_names() 

lvls <- rankings |> 
  count(rating, root_word) |> 
  filter(rating == "Strong") |>
  arrange(desc(n)) |> 
  mutate(root_word = fct_inorder(root_word)) |> 
  pull(root_word) |> 
  levels()

rankings |>
  count(rating, root_word) |> 
  mutate(root_word = factor(root_word, levels = lvls)) |> 
  filter(!is.na(root_word)) |> 
  group_by(rating) |> 
  mutate(rank = n / sum(n)) |> 
  ungroup() |> 
  drop_na(rating) |> 
  mutate(rating = factor(rating, levels = c("None", "Weak", "Moderate", "Strong"))) |> 
  ggplot(aes(x = rank, y = root_word, fill = rating)) + 
  geom_col(position = position_fill(reverse = TRUE)) +
  scale_fill_viridis_d( direction = -1)  +
  labs(y = "root word") +
  theme(axis.ticks = element_blank(), panel.grid = element_blank())
```

Instead of clear questions with obvious assumptions and goals, we end up with "Schrödinger's causal inference":

> Our results suggest that "Schrödinger's causal inference," - where studies avoid stating (or even explicitly deny) an interest in estimating causal effects yet are otherwise embedded with causal intent, inference, implications, and recommendations - is common.
>
> --- @haber_causal_language

An excellent first step to address this problem is recognizing that questions about description, prediction, and explanation are fundamentally different.
Data science in industry isn't quite as burdened by Schrödinger's causal inference as the academic sciences, but being explicit in the differences in these analyses is still helpful.
For instance, when a stakeholder asks for "drivers" of a particular event, what are they asking?
For a model to predict the event?
For a deeper understanding of what causes the event?
It's a vague request, but it smacks of causal interest to us.
When we're clear about our goals, we can use all three approaches more effectively (and, as we'll see, both descriptive analysis and prediction models are still helpful when the goal is to make causal inferences).

## Description, prediction, and explanation

### Description

The goal of descriptive analysis to describe the distribution of variables, often stratified by key variables of interest. A closely related idea is exploratory data analysis (EDA), although descriptive analyses often have clearer goals than those in EDA. That makes EDA a subset of descriptive analyses.

Descriptive analyses are usually based on statistical summaries such as measures of centrality (means, medians) and spread (minimums, maximums, quartiles), but they also occasionally use techniques like regression modeling. The goal applying more advanced techniques like regression is different in descriptive analyses than in either predictive or causal analyses. "Adjusting" for a variable in descriptive analyses means that we are removing its effect (and thus changing our question), *not* that we are controlling for confounding.

In epidemiology, a useful concept for descriptive analyses is "person, place, and time" -- who has what disease, where, and when. This is a good template for descriptive analyses in other fields, as well. Usually we want to be clear about what population we're trying to describe, and so we need to be as specific as possible. For human health, describing the people involved, the location, and the time period are all key. In other words, focus on the first principles of generating understanding about your data, and describe your data accordingly. 

#### Examples

Counting things is one of the best things we can do with data. EDA is useful for both predictive and causal analyses, but descriptive analyses are useful independent of the other types of analysis tasks. Ask any data scientist who thought they'd be developing complex machine learning models and find themselves spending most of their time on dashboards. Understanding the basic distributions of the data, particularly for key analyses goals (say, KPIs in industry or disease incidence in epidemiology) is critical for many types of decision making.

One of the best recent examples of descriptive analyses is the COVID-19 pandemic. In 2020, particularly in the early months of the pandemic, descriptive analyses were key to understanding risk and allocating resources. Since the coronavirus has some similarities to other types of respiratory diseases, we had many public health tools to use (e.g. distancing, and later, face masks). Descriptive statistics of cases by region were important for deciding local policies and the strength of those policies. 

A great example of a more complex descriptive analysis in the pandemic was an ongoing report by the Financial Times of expected deaths vs. observed deaths in various countries and regions. While the calculation of expected deaths is slightly more sophisticated than most descriptive statistics, it provided a tremendous amount of information about current deaths without needing to untangle causal effects (e.g., were they due to COVID-19 directly? Inaccessible healthcare? Cardiovascular events post-COVID?).

TODO: include plot

Here are some other great examples of descriptive analyses.

TODO: include examples

#### Validity 

There are two key issues of validity in descriptive analyses: measurement error and sampling error. 

Measurement error is when we have mismeasured one or more variables in some capacity. For descriptive analyses, mismeasuring things means we may not be getting the actual answer to our question. However, the degree to which that is the case depends on both the severity of the measurement error and the question itself.

Sampling error is a more nuanced topic in descriptive analyses. It's related to both the population we're analyzing (who should the analysis produce descriptions about) and uncertainty (how certain are we that the descriptions of those we have data for represent the population we're trying to describe.).  

The population from which our data come and the population we're trying to describe must be the same for us to provide valid descriptions. Consider a dataset generated by an online survey. Who are the people who are answering these questions, and how do they relate to the people we want to describe? For many analyses, the people who take the time to answer surveys are different than the people we want to make descriptions about, e.g. the group of people who fill out surveys may have a different distribution of variables than people who don't. This is not technically bias because, outside of sample size-related uncertainty and measurement error, the descriptions are accurate---they're just not for the right group of people! In other words, we've gotten an answer to the wrong question.

Notably, sometimes our data represent the entire  population (or close enough) that sampling error is irrelevant. Consider a company that has certain types of data on every customer that uses their service. For many useful analyses, this represents the entire population (current customers) about whom we want information. Similarly, in countries with population-wide health registries, the data available, for certain practical purposes, is close enough to the entire population that no sampling is needed (although researchers might use sampling for simpler computations). In these cases, there's not really such a thing as uncertainty. Assuming everything is well-measured, the summary statistics we generate *are inherently unbiased and precise* because we have information from everyone in the population. Of course, in practice, we usually have some mixture of measurement error, missing data, and so on, even in the best of circumstances.

One important detail of descriptive analysis is that confounding bias, one of the chief concerns of this book, is undefined. That's because confounding is a causal concern. Descriptive analyses cannot be confounded because they are a statistical description of relationships as-is, not the mechanisms behind those relationships.

#### Relationship to causal inference

Humans are built to see patterns; we can't help but see them, and this is particularly true in data analysis. This is a really helpful feature of our brains, but it can also lead down a slippery slope of inference when we're not working with data or methods that can allow us to do that validly. The single biggest thing to be cautious of when your goal is to describe is making the leap from description to causation (implicitly or explicitly). 

But of course, descriptive analysis is useful too when we *are* estimating causal effects. It helps us understand the population we're working with, the distribution of the outcomes, exposures (variables we think might be causal), and confounders (variables we need to account for to get unbiased causal effects for the exposure). You should always do descriptive analyses of your data when you are conducting a causal analysis. 

Finally, as we'll see in [Chapter -@sec-trials-std], there are certain circumstances where we can make causal inferences with basic statics. Be cautious about the distinction between the causal question and the descriptive component here, too, though: just because we're using the same calculation (e.g., a difference in means) doesn't mean that all descriptions you can generate are causal in nature.

### Prediction

The goal of prediction is to use data to make accurate predictions about variables, usually on new data. What exactly this means depends on the question, domain, and so on. Prediction models are used in about every setting imaginable, from peer-reviewed clinical models to bespoke machine learning models embedded in consumer devices. Even large language models like the ones ChatGPT is based on are prediction models: they predict what a response to a prompt would look like.

Predictive modeling generally uses a different workflow than the workflow for causal modeling we'll present in this book. Since the goal of prediction is usually related to making predictions on new data, the workflow of this type of modeling focuses on maximizing predictive accuracy while retaining generalization to new data, sometimes called the bias-variance trade-off. In practice, this means splitting your data into training sets (the part of the data you build your model on) and test sets (the part of the data you assess your model with, a proxy for how it would perform on new data). Usually, data scientists use cross-validation or other sampling techniques to further reduce the risk of overfitting your model to the training set. 

There are many excellent texts on predictive modeling, and so we refer you to those for a deeper exploration of the goals and methods of these techniques TODO: cite

#### Examples

TODO: include main example. Covid predictions? 

TODO: include examples

#### Validity 

The key measure of validity in prediction modeling is predictive accuracy, which can be measured a number of ways, such as TODO: expand RMSE, AUC. A convenient detail about predictive modeling is that we can often assess if we're right, which is not true of descriptive statistics for which we only have a subset of data or causal inference for which we don't know the true causal structure. This isn't always true, but it's almost always required for fitting the initial model^1.

Measurement error is also a concern for predictive modeling because we usually need accurate data for accurate predictions. Interestingly, measurement error and missingness can be informative. In a causal setting, this might induce bias, but predictive models can consume that information with no issue. For instance, in the famous Netflix Prize, winning models leveraged information about whether or not a customer rated a movie at all to improve recommendation systems. 

Like descriptive analysis, confounding is undefined for predictive modeling. A coefficient in a prediction model cannot be confounded, and we usually only care about whether or not the variable provides predictive information, not if that information is because of a causal relationship or something else.

[^1 We say model singular but usually data scientists fit many models for experimentation and often the best prediction models are some combination of predictions from several models, called a stacked model]

#### Relationship to causal inference

The single biggest risk in prediction is to assume that a given coefficient in a model has a causal interpretation. There is a good chance that this isn't so. A model may predict well but may also have completely biased coefficients from a causal point of view. We'll see more about this in @sec-pred-or-explain and, indeed, the rest of the book.

Often, people mistakenly use methods for selecting features (variables) for prediction models to select confounders in causal models. These methods, aside from their risk of overfitting, are appropriate for prediction models but not for causal model. Prediction metrics cannot determine the causal structure of your question, and predictive value for the outcome does not make a variable a confounder. We'll discuss methods for choosing variables for causal models in [Chapter -@sec-dags].

Prediction is nevertheless crucial to causal inference. From a philosophical perspective, we're comparing predictions from different *what if* scenarios: what would the outcome had one thing happened vs if another thing happened? We'll spend a lot of time on this subject, particularly in [Chapter -@sec-counterfactuals]. We'll also talk a lot about prediction from a practical perspective: just like in prediction and some description, we'll use modeling techniques to answer causal questions. Techniques like propensity score methods and g-computation use model predictions to answer causal questions, but the workflow for building models---and the interpretation of the models themselves---are different.

### Causal Inference

The goal of causal inference is to understand the impact that a variable, sometimes called an exposure, has on another variable, sometimes called an outcome. "Exposure" and "outcome" are the terms we'll use in this book to describe the causal relationship we're interested in. Importantly, our goal is to answer this question in a clear, precise way. In practice, this means using techniques like study design (e.g. a randomized trial) or statistical methods (like propensity scores) to calculate an unbiased effect of the exposure on the outcome.

As with prediction and description, it's better to start with a clear, precise question to get a clear, precise answer. In statistics and data science, particularly as we swim through the ocean of data of the modern world, we often end up with an answer without a question (e.g., `42`). This, of course, makes interpretation of the answer difficult. In @sec-diag, we'll discuss the nature of causal questions. In [Chapter -@sec-counterfactuals] and [Chapter -@sec-trials-std], we'll discuss philosophical and practical ways, respectively, to sharpen the questions we have.

#### Examples

TODO: include main example. face masks study by ellie? 

TODO: include examples

#### Validity 

Making valid causal inferences requires several assumptions that we'll discuss in @sec-assump. As opposed to prediction, we generally cannot confirm that our causal models are correct. In other words, most of the assumptions we need to make are unverifiable. We'll come back to this topic time and time again in the book---from the basics of these assumptions, to practical decision-making, to probing our models for problems. 

### Why isn't the best causal model just the best prediction model? {#sec-pred-or-explain}

You may wonder at this point: why isn't the best causal model just the best prediction model? It makes sense that the two would be related: naturally, things that cause other things would be predictors. Doesn't it stand to reason that predictors would be causal in nature, too? Unfortunately, this is not always the case; causal effects needn't predict particularly well and good predictors needn't be causal. Most importantly, there is no way to distinguish these situations using data alone.

Let's consider the causal perspective first, because it's a bit simpler: if an outcome has many causes, then a model that accurately describes such relationship likely won't predict the outcome very well. Likewise, if a true causal effect is small, it won't bring much predictive value. Of course, low predictive power might also be a sign that a causal effect isn't much use from an applied perspective, although that depends on number of statistical factors.

There are two slightly more complex reasons that predictive models won't always be good causal models. For the first reason, let's consider a model that is accurate from a causal perspective: it estimates effects on an outcome, and all of these effects are unbiased. Even in this ideal setting, you might be able to get better predictions by using a different model. The reason has to do with the bias-variance trade-off in predictive modeling. When effects are small, data are noisy, or TODO: ADD OTHER REASONS, then it might make sense to use a biased model like penalized regression. These types of models intentionally introduce bias in favor of improved variance in out-of-data predictions. Since the goals of predication and causal inference are different (accurate predictions, usually for out-of-data observations vs. an unbiased effect), the best model for inference is not necessarily the best model for prediction.

Secondly, variables which are bias from a causal perspective often bring along with them predictive power. We'll talk more about which variables to include and *not* include in your models in @sec-dags, but let's consider a simple example. One of the famous examples of confounded relationships is ice cream sales and crime in the summer. Descriptively, ice cream sales and crime are related, but this relationship is confounded by weather, e.g. both ice cream sales and crime increases when it's warmer. Consider a thought experiment where you are in a dark room. Your goal is to predict crime, but you don't know the weather or time of year. You do, however, have information on ice cream sales. A model with ice cream sales on crime would be biased from a causal perspective---ice cream sales do not cause crime, but the model would show an effect---but would provide some predictive value to your crime model. The reason for both of these conditions is the same: weather and ice cream sales are correlated, and so are weather and crime. So, ice cream sales can successfully, if imperfectly, serve as a proxy for weather. That results in a biased effect estimate of the causal impact of ice cream sales on crime but a partially effective prediction of crime. Other variables, too, which are invalid from a causal perspective, either by being biased themselves or by introducing bias into the causal effect estimate, often bring good predictive value. Thus predictive accuracy is not a good measure of causality.

A closely related idea is the *Table Two Fallacy*, so-called because in health research papers, descriptive analyses are often presented in Table 1 and regression models are often presented in Table 2. The Table Two Fallacy is when a researcher presents confounders and other non-effect variables and, particularly, when they interpret those coefficients as if they, too, were causal. The problem is that in some situations, the model to estimate an unbiased effect of one variable is not the same model to estimate an unbiased effect of another variable in the model. In other words, we can't interpret the effects of confounders as causal because they might *themselves* be confounded by another variable unrelated to the original exposure. 

Descriptive, predictive, and causal analyses will always contain some aspect of each other. A predictive model leans on causality in some aspect, and a causal model with have some predictive power. However, the same model in the same data with different goals with have different usefulness depending on those goals. Be cautious and be thoughtful.

## Diagraming a causal claim {#sec-diag}

Diagramming sentences is a grammatical method used to visually represent the structure of a sentence, occasionally taught in grammar school.
In this technique, sentences are deconstructed into their constituent parts, such as subjects, verbs, objects, and modifiers, and then displayed using a series of lines and symbols.
The arrangement of these elements on the diagram reflects their syntactic roles and how they interact within the sentence's overall structure.
By breaking down sentences into these visual representations, diagramming can help learners grasp the nuances of sentence construction, identify grammatical errors, and appreciate the intricate connections between words.
We can apply a similar idea to *causal claims*.
Here is an example of how one might diagram a causal claim.
We've pulled out the *cause*, the *effect*, the *subject* (for whom?), and the *timing* (when?).


```{r}
#| echo: false
#| fig-cap: "Example of diagraming a causal claim"
#| fig-height: 2
#| label: fig-diagram-1
library(ggplot2)

data <- data.frame(labels = c("cause", "effect", "for whom?", "when?", "for whom?", "when?"),
                   x = c(1.25, 1.75, 1.25, 1.55, 1.8, 2.05),
                   y = c(1, 1, 0.8, 0.7, 0.8, 0.7),
                   angle = c(0, 0, -30, 0, -30, 0))  

ggplot(data, aes(x = x, y = y)) +
  geom_text(aes(label = labels, angle = angle, vjust = 0), 
            size = 7) +
  geom_segment(aes(x = 1, xend = 2, y = 0.95, yend = 0.95)) +  
  geom_segment(aes(x = 1.5, xend = 1.5, y = 0.95, yend = 1.1)) +
  geom_segment(aes(x = 1, xend = 1.35, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1.35, xend = 1.65, y = 0.65, yend = 0.65)) +
  geom_segment(aes(x = 1.55, xend = 1.9, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1.9, xend = 2.15, y = 0.65, yend = 0.65)) +
  theme_void()
```

Let's start with a basic causal question: **Does smoking cause cancer?**

The causal claim here could be *smoking causes cancer*.
@fig-diagram-2 shows a potential diagram of this causal claim.

```{r}
#| echo: false
#| label: fig-diagram-2
#| fig-height: 2
#| fig-cap: "Diagram of the causal claim \"smoking causes cancer\"."
data <- data.frame(labels = c("smoking", "lung cancer", "for everyone?", "immediately?", "for everyone?", "immediately?"),
                   x = c(1.25, 1.75, 1.25, 1.55, 1.8, 2.05),
                   y = c(1, 1, 0.8, 0.7, 0.8, 0.7),
                   angle = c(0, 0, -30, 0, -30, 0))  

ggplot(data, aes(x = x, y = y)) +
  geom_text(aes(label = labels, angle = angle, vjust = 0), 
            size = 6) +
  geom_segment(aes(x = 1, xend = 2, y = 0.95, yend = 0.95)) +  
  geom_segment(aes(x = 1.5, xend = 1.5, y = 0.95, yend = 1.1)) +
  geom_segment(aes(x = 1, xend = 1.35, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1.35, xend = 1.65, y = 0.65, yend = 0.65)) +
  geom_segment(aes(x = 1.55, xend = 1.9, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1.9, xend = 2.15, y = 0.65, yend = 0.65)) +
  theme_void()
```

Let's try to get more specific.
A study was published in *JAMA* (the Journal of the American Medical Association) in 2005 titled "Effect of Smoking Reduction on Lung Cancer Risk".
This study concluded: "Among individuals who smoke 15 or more cigarettes per day, smoking reduction by 50% significantly reduces the risk of lung cancer".
[@godtfredsen2005effect] The study goes on to describe the time frame studied as 5-10 years.
Let's diagram this causal claim.
Here, we are assuming that the eligibility criteria and the target population for the estimated causal effect are the same (individuals who smoke 15 or more cigarettes per day), this need not always be the case.
In @sec-estimands we will discuss other potential target populations.

```{r}
#| echo: false
#| fig-cap: "Example diagram of a more specific causal claim based on results from @godtfredsen2005effect"
#| label: fig-diagram-3

data <- data.frame(labels = c("Reducing smoking by 50%", "Reduced lung cancer", "People who\nsmoke 15+\ncigarettes per day", "when they stopped", "People who\nsmoke 15+\ncigarettes per day", "between 5-10 years"),
                   x = c(1, 2, .83, 1.4, 1.88, 2.45),
                   y = c(1, 1, 0.77, 0.7, 0.77, 0.7),
                   angle = c(0, 0, -52, 0, -52, 0))  

ggplot(data, aes(x = x, y = y)) +
  geom_text(aes(label = labels, angle = angle, vjust = 0), size = 5) +
  geom_segment(aes(x = 0.5, xend = 2.5, y = 0.95, yend = 0.95)) +  
  geom_segment(aes(x = 1.5, xend = 1.5, y = 0.95, yend = 1.1)) +
  geom_segment(aes(x = 0.5, xend = 1, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1, xend = 1.5, y = 0.65, yend = 0.65)) +
  geom_segment(aes(x = 1.55, xend = 2.05, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 2.05, xend = 2.55, y = 0.65, yend = 0.65)) +
  xlim(c(0.5, 2.75)) +
  ylim(c(0.5, 1.2)) +
  theme_void() 
```

Translating this idea into asking good causal questions, we can map the following terms that you will see throughout this book to these diagrams: *exposure* (the cause), *outcome* (the effect), *eligibility criteria* (for whom?), *time zero* (when did the participants begin to be followed?), *target population*, (who can we estimate an outcome effect for?) and *follow up period* (when?).

```{r}
#| echo: false
#| label: fig-diagram-4
#| fig-height: 2
#| fig-cap: "Example diagram mapped to causal analysis terminology"
data <- data.frame(labels = c("exposure", "outcome", "eligibility criteria", "time zero", "target population", "follow-up time"),
                   x = c(1.25, 1.75, 1.25, 1.55, 1.8, 2.15),
                   y = c(1, 1, 0.8, 0.7, 0.8, 0.7),
                   angle = c(0, 0, -30, 0, -30, 0))  

ggplot(data, aes(x = x, y = y)) +
  geom_text(aes(label = labels, angle = angle, vjust = 0), 
            size = 7) +
  geom_segment(aes(x = 1, xend = 2, y = 0.95, yend = 0.95)) +  
  geom_segment(aes(x = 1.5, xend = 1.5, y = 0.95, yend = 1.1)) +
  geom_segment(aes(x = 1, xend = 1.35, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1.35, xend = 1.65, y = 0.65, yend = 0.65)) +
  geom_segment(aes(x = 1.55, xend = 1.9, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1.9, xend = 2.15, y = 0.65, yend = 0.65)) +
  theme_void()
```

Asking good causal questions means we map the *question* to the observable *evidence*.
Let's return to the smoking example.
Our initial question was: *Does smoking causes lung cancer?*; using our study, the evidence shows *For people who smoke 15+ cigarettes a day, reducing smoking by 50% reduces the risk of lung cancer over 5-10 years*.
Do these match?
Not quite.
Let's update our question to match what the study actually was able to show: *For people who smoke 15+ cigarettes a day, does reducing smoking by 50% reduce the lung cancer risk over 5-10 years?*.
Honing this skill, asking answerable causal questions, is important, and one that we will discuss throughout this book.
