# Building propensity score models

Often we are interested in how some *exposure* (or treatment) impacts an outcome.
For example, we could assess how an ad campaign (exposure) impacts sales (outcome), whether a certain medication (exposure) improves patient survival (outcome), or whether opening a theme park early to some visitors (exposure) reduces wait times later in the day (outcome).
As defined in the previous chapter, an exposure in the context of this book is often a modifiable event or condition that occurs prior to the outcome.
In randomized trials, participants are randomly assigned to exposure groups.
If all goes well, this allows for an unbiased estimate of the causal effect between the exposure and outcome.
In the "real world", outside this randomized trial setting, we are often *exposed* to something based on other factors.
For example, when deciding what medication to give a diabetic patient, a doctor may consider the patient's medical history, their likelihood to adhere to certain medications, and the severity of their disease.
The medication they are given is no longer random; it is *conditional* on factors about that patient, also known as the patient's *covariates*.
If these covariates also affect the outcome, they are *confounders*.
If we could collect information about all of these factors, we could determine each patient's probability of exposure and use this to inform an analysis assessing the relationship between that exposure and some outcome.
This is the propensity score!
When fitting a *propensity score model* we want to condition on all known confounders.

::: def-box
A **propensity score** is the probability of being in the exposure group, conditioned on observed covariates.
:::

@rosenbaum1983central showed in observational studies, conditioning on propensity scores can lead to unbiased estimates of the exposure effect as long as certain assumptions hold:

1.  There are no unmeasured confounders
2.  Every subject has a nonzero probability of receiving either exposure

## Logistic Regression

There are many ways to estimate the propensity score; typically people use logistic regression for binary exposures.
This is done by fitting a logistic regression model predicting the exposure using known confounders.
Each individual's predicted value is the propensity score.
In R, a logistic regression model can be fit using the `glm()` function.
Below is pseudo-code.
The first argument is the model, with the exposure on the left side and the confounders on the right.
The data frame is passed to the `data` argument and the `family = binomial()` argument to denote the model should be fit using logistic regression (as opposed to a different generalized linear model).

```{r, eval = FALSE}
glm(
  exposure ~ confounder_1 + confounder_2,
  data = df,
  family = binomial()
)
```

We can extract the propensity scores by pulling out the predictions on the probability scale.
Using the `augment()` function from the broom package, we can extract these propensity scores and add them to our original data frame.
The argument `type.predict` is set to `"response"` to indicate that we want to extract the predicted values on the *probability* scale.
By default, these will be on the linear logit scale.
The `data` argument contains the original data frame.
This code will output a new data frame consisting of all components in `df` with six additional columns corresponding to the logistic regression model that was fit.
The `.fitted` column is the propensity score.

```{r, eval = FALSE}
glm(
  exposure ~ confounder_1 + confounder_2,
  data = df,
  family = binomial()
) %>%
  augment(type.predict = "response", data = df)
```

Let's look at an example.

### Example

Historically, guests who stayed in a Walt Disney World resort hotel were able to access the park during "Extra Magic Hours" during which the park was closed to all other guests.
These extra hours could be in the morning or evening.
The Seven Dwarfs Mine Train is a ride at Walt Disney World's Magic Kingdom.
Typically, each day Magic Kingdom may or may not be selected to have these "Extra Magic Hours".
We are interested in examining the relationship between whether there were "Extra Magic Hours" in the morning and the average wait time for the Seven Dwarfs Mine Train the same day between 5pm and 6pm.
Below is a proposed DAG for this question.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Proposed DAG for the relationship between Extra Magic Hours in the morning at a particular park and the average wait time between 5pm and 6pm."}
library(tidyverse)
library(ggdag)

coord_dag <- list(
  x = c(Season = 0, close = 0, weather = -1, x = 1, y = 2),
  y = c(Season = -1, close = 1, weather = 0.25, x = 0, y = 0)
)

labels <- c(
  x = "Extra Magic Morning",
  y = "Average wait",
  Season = "Ticket Season",
  weather = "Historic high temperature",
  close = "Time park closed"
)

dagify(
  y ~ x + close + Season + weather,
  x ~ weather + close + Season,
  coords = coord_dag,
  labels = labels
) %>%
  ggdag(use_labels = "label", text = FALSE) +
  theme_void() +
  scale_x_continuous(
    limits = c(-1.25, 2.25), 
    breaks = c(-1, 0, 1, 2), 
    labels = c("\n(one year ago)", "\n(6 months ago)", "\n(3 months ago)", "5pm - 6pm\n(Today)")
  ) +
  theme(axis.text.x = element_text())
```

Here we are proposing that there are three confounders: the historic high temperature on the day, the time the park closed, and the ticket season: value, regular, or peak.
We can build a propensity score model using the `seven_dwarfs_train_2018` data set from the touringplans package.
Each row of this dataset contains information about the Seven Dwarfs Mine Train during a certain hour on a given day.
First we need to subset the data to only include average wait times between 5 and 6 pm.
Then we will use the `glm()` function to fit the propensity score model, predicting `extra_magic_morning` using the four confounders specified above.
We'll add the propensity scores to the data frame (in a column called `.fitted` as set by the `augment()` function in the broom package).

```{r}
library(broom)
library(touringplans)

seven_dwarfs <- seven_dwarfs_train_2018 %>%
  filter(hour == 17)

seven_dwarfs_with_ps <- glm(
  extra_magic_morning ~ wdw_ticket_season + close + weather_wdwhigh,
  data = seven_dwarfs,
  family = binomial()
) %>%
  augment(type.predict = "response", data = seven_dwarfs)
```

Let's take a look at these propensity scores.
Below, the propensity scores are shown in the `.fitted` column for the first 10 observations.
The propensity score here is the probability that a given date will have Extra Magic Hours in the morning given the observed confounders, in this case the historical high temperatures on a given date, the time the park closed, and Ticket Season.
For example, on January 1, 2018 there was a 30.4% chance that there would be Extra Magic Hours at the Magic Kingdom given the Ticket Season (peak in this case), time of park closure (11pm), and the historic high temperature on this date (70.3 degrees).
On this particular day, there were *not* Extra Magic Hours in the morning (as indicated by the 0 in the first row of the `extra_magic_morning` column).

```{r}
seven_dwarfs_with_ps %>%
  select(date, extra_magic_morning, wdw_ticket_season, close, weather_wdwhigh, .fitted)
```

We can examine the distribution of propensity scores by exposure group.
A nice way to visualize this is via "mirrored histograms".
The code below creates two histograms of the propensity scores, one on the "top" for the exposed group (the dates with Extra Magic Hours in the morning) and one on the "bottom" for the unexposed group.
Because we only have two variables

```{r}
ggplot() +
  geom_histogram(
    data = seven_dwarfs_with_ps %>% filter(extra_magic_morning == 1),
    aes(x = .fitted),
    fill = "orange",
    bins = 40
  ) +
  geom_histogram(
    data = seven_dwarfs_with_ps %>% filter(extra_magic_morning == 0),
    aes(
      x = .fitted,
      y = -after_stat(count)
    ),
    fill = "cornflower blue",
    bins = 40
  ) +
  scale_y_continuous("Count", labels = abs) +
  scale_x_continuous("Propensity Score")
```

In the code above, the first `geom_histogram()` call creates the top histogram by subsetting the `seven_dwarfs_with_ps` data frame to only include those in the exposure group, where `extra_magic_morning == 1`.
The x-axis for the histogram is the propensity score, which lives in the `.fitted` column of the data frame.
The second `geom_histogram()` call creates the bottom histogram, subsetting to only those in the unexposed group.
Here, we use the `y` argument to *flip* the histogram by setting this to `-after_stat(count)`.
`after_stat()` allows you to work with values created internally by ggplot2, such as `count`, the frequency counts for the histogram.
Finally, we need to edit the y-axis to show the absolute value (rather than negative values below 0) so the correct numbers are displayed; this is accomplished by setting `label = abs` in the `scale_y_continuous()` function.

## Continuous exposures

Propensity scores generalize to many other types of exposures, including continuous exposures.
At its heart, the workflow is the same: we fit a model where the exposure is the outcome then use that model to weight a second outcome model.
For continuous exposures, linear regression is the simplest way to create propensities.
Instead of probabilities, we use the cumulative density function.
Then, we use this density to weight the outcome model.

Let's take a look at an example.
In the `tourplans` data set, we have information about the posted waiting times for rides.
We also have a limited amount of data on the observed, actual times.
The question that we will consider is this: Do posted wait times for the Seven Dwarves Mine Train at 8 am affect actual wait times at 9 am?
Here's our DAG:

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Proposed DAG for the relationship between posted wait in the morning at a particular park and the average wait time between 5pm and 6pm."}
library(tidyverse)
library(ggdag)

coord_dag <- list(
  x = c(Season = -1, close = -1, weather = -2, extra = 0, x = 1, y = 2),
  y = c(Season = -1, close = 1, weather = 0.25, extra = 0, x = 0, y = 0)
)

labels <- c(
  extra = "Extra Magic Morning",
  x = "Average posted wait ",
  y = "Average acutal wait",
  Season = "Ticket Season",
  weather = "Historic high temperature",
  close = "Time park closed"
)

dagify(
  y ~ x + close + Season + weather + extra,
  x ~ weather + close + Season + extra,
  coords = coord_dag,
  labels = labels
) %>%
  ggdag(use_labels = "label", text = FALSE) +
  theme_void() +
  scale_x_continuous(
    limits = c(-2.25, 2.25), 
    breaks = c(-2, -1, 0, 1, 2), 
    labels = c("\n(one year ago)", "\n(6 months ago)", "\n(3 months ago)", "8am-9am\n(Today)", "9am-10am\n(Today)")
  ) +
  theme(axis.text.x = element_text())
```

We're assuming that our primary confounders are the time the park closes, historic high temperatures, whether or not the ride has extra magic morning hours, and the ticket season.
The confounders precede the exposure and outcome, and (by definition) the exposure precedes the outcome.
The average posted wait time is, in theory, a manipulable exposure because the park could post a time different from what they expect.

The model is similar to the binary exposure case, but we need to use linear regression, as the posted time is a continuous variable.
Since we're not using probabilities, we'll instead calculate denominators for our weights from a normal density.
We then calculate the denominator using the `dnorm()` function, which calculates the normal density for the `exposure`, using `.fitted` as the mean and `mean(.sigma)` as the SD.

```{r, eval = FALSE}
lm(
  exposure ~ confounder_1 + confounder_2,
  data = df
) %>%
  augment(data = df) %>% 
  mutate(denominator = dnorm(exposure, .fitted, mean(.sigma, na.rm = TRUE)))
```

Continuous exposure weights, however, are very sensitive to modeling choices.
One problem particular is the existence of extreme weights, an issue that can also affect other types of exposures.
When some observations have extreme weights, the propensities are *destabilized.* We can stabilize them using the marginal distribution of the exposure.
A common way to calculate the marginal distribution for propensity scores is to use a regression model with no predictors:

```{r, eval = FALSE}
# for continuous exposures
lm(
  exposure ~ 1,
  data = df
) %>%
  augment(data = df) %>% 
  transmute(numerator = dnorm(exposure, .fitted, mean(.sigma, na.rm = TRUE)))

# for binary exposures
glm(
  exposure ~ 1,
  data = df,
  family = binomial()
) %>%
  augment(type.predict = "response", data = df) %>% 
  select(numerator = .fitted)
```

Then, rather than inverting them, we calculate the weights as `numerator / denominator`.
Let's try it out on our posted wait times example.
First, let's wrangle our data to address our question: do posted wait times at 8 affect actual weight times at 9?
We'll join the baseline data (all covariates and posted wait time at 8) with the outcome (average actual time).
We also have a lot of missingness for `avg_sactmin`, so we'll drop unobserved values for now.

```{r}
eight <- seven_dwarfs_train_2018 %>% 
  filter(hour == 8) %>% 
  select(-avg_sactmin)

nine <- seven_dwarfs_train_2018 %>% 
  filter(hour == 9) %>% 
  select(date, avg_sactmin)

wait_times <- eight %>% 
  left_join(nine, by = "date") %>% 
  drop_na(avg_sactmin)
```

First, let's calculate our denominator model.
We'll fit a model using `lm()` for `avg_spostmin` with our covariates, then use the fitted predictions of `avg_spostmin` (`.fitted`) to calculate the density using `dnorm()`.

```{r}
denominator_model <- lm(
  avg_spostmin ~ close + extra_magic_morning + weather_wdwhigh + wdw_ticket_season, 
  data = wait_times
)

denominators <- denominator_model %>% 
  augment(data = wait_times) %>% 
  mutate(denominator = dnorm(avg_spostmin, .fitted, mean(.sigma, na.rm = TRUE))) %>% 
  select(date, denominator)
```

When we just use the inverted values of `denominator`, we end up with several extreme weights:

```{r}
denominators %>% 
  mutate(wts = 1 / denominator) %>% 
  ggplot(aes(wts)) +
  geom_density(col = "#E69F00", fill = "#E69F0095") + 
  scale_x_log10() + 
  theme_minimal(base_size = 20) + 
  xlab("Weights")
```

Let's now fit the marginal density to use for stabilized weights:

```{r}
numerator_model <- lm(
  avg_spostmin ~ 1, 
  data = wait_times
)

numerators <- numerator_model %>% 
  augment(data = wait_times) %>% 
  mutate(numerator = dnorm(avg_spostmin, .fitted, mean(.sigma, na.rm = TRUE))) %>% 
  select(date, numerator)
```

We also need to join the fitted values back to our original data set by date, then calculate the stabilized weights (`swts`) using `numerator / denominator`.

```{r}
wait_times_wts <- wait_times %>% 
  left_join(numerators, by = "date") %>% 
  left_join(denominators, by = "date") %>% 
  mutate(swts = numerator / denominator)
```

The stabilized weights are much less extreme.
Often, stabilized weights have a mean close to 1; when that is the case, then the psuedo-population (that is, the equivalent number of observations after weighting) is equal to the original sample size.

```{r}
ggplot(wait_times_wts, aes(swts)) +
  geom_density(col = "#E69F00", fill = "#E69F0095") + 
  scale_x_log10() + 
  theme_minimal(base_size = 20) + 
  xlab("Stabilized Weights")
```

TODO: Label scatterplot of weights vs exposure

TODO: discuss `mean(wait_times_wts$swts)` -- what does Causal Inference say?
1 in the linear case, not in others?
Stabilizing with other numerators?

TODO: Mention binary weights that don't have this problem because they're bounded

TODO: Why do we need to stabilize?
Why are extreme weights a problem?

**TODO**: **is there a good next visualization for the continuous case like the mirrored histogram above?**

TODO: [https://disneyparks.disney.go.com/blog/2018/06/disney-doodle-pluto-sniffs-out-fun-at-seven-dwarfs-mine-train](https://disneyparks.disney.go.com/blog/2018/06/disney-doodle-pluto-sniffs-out-fun-at-seven-dwarfs-mine-train/)

TODO: Does weighting not have a problem with colliders?

## Choosing what variables to include

**TODO:** add references

The best way to decide what variables to include in your propensity score model is to look at your DAG and have at least a minimal adjustment set of confounders.
Of course, sometimes, essential variables are missing or measured with error.
In addition, there is often more than one theoretical adjustment set that debiases your estimate; it may be that one of the minimal adjustment sets is measured well in your data set and another is not.
If you have confounders on your DAG that you do not have access to, sensitivity analyses can help quantify the potential impact.
See Chapter 11 for an in-depth discussion of sensitivity analyses.

Accurately specifying a DAG improves our ability to add the correct variables to our models.
However, confounders are not the only necessary type of variable to consider.
For example, variables that are predictors of the *outcome* *but not the exposure* can improve the precision of propensity score models.
Conversely, including variables that are predictors of the *exposure but not the outcome* (instrumental variables) can bias the model.
Luckily, in practice, it seems that this bias is relatively negligible, especially compared to the risk of confounding bias.

Another type of variable to be wary of is a *collider*, a variable that is a descendant of both the exposure and the outcome.
If you specify your DAG correctly, you can avoid colliders by only using adjustment sets that completely close backdoor paths from the exposure to the outcome.
However, some circumstances make this difficult: some colliders are inherently stratified by the study's design or the nature of follow-up.
For example, loss-to-follow-up is a common source of collider-stratification bias; in Chapter XX, we'll discuss this further.

A variable can also be both a confounder and a collider, as in the case of so-called butterfly bias:

```{r}
ggdag(butterfly_bias())
```

If we have all the variables measured well, we can avoid adjusting form by adjusting for either `a` or `b`.
However, what should we do if we don't have those variables?
Adjusting for `m` opens a biasing pathway that we cannot block through `a` and `b` (collider stratification bias), but `m` is also a confounder for `x` and `y`.
As in the case above, it appears that confounding bias is often the worse of the two options, so we should adjust for `m` unless we have reason to believe it will cause more problems than it solves.

### Don't use prediction metrics for causal modeling

By and large, metrics commonly used for building prediction models are inappropriate for building causal models.
Researchers and data scientists often make decisions about models using metrics like R^2^, AUC, accuracy, and (often inappropriately) p-values.
However, a causal model's goal is not to predict as much about the outcome as possible; the goal is to accurately estimate the relationship between the exposure and outcome.
A causal model needn't predict particularly well to be unbiased.

Where these metrics serve a role is identifying the best *functional form* of a model.
Generally, we'll use DAGs and our domain knowledge to build the model itself.
However, we may be unsure of the mathematical relationship between a confounder and the outcome or exposure.
For instance, we may not know if the relationship is linear.
Misspecifying this relationship can lead to residual confounding: we may only partially account for the confounder in question, leaving some bias in the estimate.
Testing different functional forms using prediction-focused metrics can help improve the accuracy of the model itself, potentially allowing for better control.

Another technique researchers sometimes use to determine confounders is to add a variable and determine the percent change in the coefficient between the outcome and exposure.
For instance, we first model `y ~ x` to estimate the relationship between `x` and `y`.
Then, we model `y ~ x + z` and see how much the coefficient on `x` has changed.
A common rule is to add a variable if it changes the coefficient of `x` by 10%.

Unfortunately, this technique is unreliable.
As we've discussed, controlling for mediators, colliders, and instrumental variables all affect the estimate of the relationship between `x` and `y`, and usually they result in bias.
In other words, there are many other types of variables besides confounders that can cause a change in the coefficient of the exposure.
As we discuss above, confounding bias is often the most important factor, but systematically searching your variables for anything that changes the coefficient of the exposure can compound many types of bias.
