# Expressing causal questions as DAGs {#sec-dags}

{{< include 00-setup.qmd >}}

## Visualizing Causal Assumptions

> Draw your assumptions before your conclusions --@hernan2021

Causal diagrams are a tool to visualize your assumptions about the causal structure of the questions you're trying to answer. In a randomized experiment, the causal structure is quite simple: while there may be many causes of an outcome, the only cause of the exposure is the randomization process itself (we hope!). In many non-randomized settings, however, the structure of your question can be a complex web of causality. Causal diagrams help communicate what we think this structure looks like. In addition to being open about what we think the causal structure is, causal diagrams have incredible mathematics properties that allow us to identify a way to estimate unbiased causal effects even with observational data.

The type of causal diagrams we use are also called directed acyclic graphs (DAGs). These graphs are directed because they include arrows going in a specific direction. They're acyclic because they don't go in circles; a variable can't cause itself, for instance. DAGs are used for a wide variety of problems, but we're specificaly concerned with *causal* DAGs. This class of DAGs is also sometimes referred to as Structural Causal Models (SCMs) because they are a model of the causal structure of a question. 

::: {.callout-tip}
## DAGs down under

We highly recommend asking an Australian friend about DAGs. 
:::

DAGs depict causal relationships between variables. Visually, the way they depict variables is as *edges* and *nodes*. Edges are the arrows going from one variable to another, also sometimes called arcs or just arrows. Nodes are the variables themselves, sometimes called vertices, points, or just variables. in @fig-dag-basic, there are two nodes: `x` and `y` and one edge going from `x` to `y`. Here, we are saying that `x` causes `y`. In some capacity, `y` "listens" to `x`.

```{r}
#| echo: false
#| message: false
#| label: fig-dag-basic
#| fig-width: 4
# TODO: add caption
library(ggdag)
dagify(y ~ x, coords = time_ordered_coords()) |> 
  ggdag() + 
  theme_dag()
```

If we're interested in the causal effect of `x` on `y`, we're trying to estimate a numeric representation of that arrow. Usually, though, there are many other variables and arrows in the causal structure of a given question. A series of arrows is called a *path*. There are three types of paths you'll see in DAGs: forks, chains, and colliders (sometimes called inverse forks).


```{r}
#| echo: false
#| label: fig-dag-path-types
# TODO: add caption
coords <- list(x = c(x = 0, y = 2, q = 1), y = c(x = 0, y = 0, q = 1))

fork <- dagify(
  x ~ q,
  y ~ q,
  exposure = "x",
  outcome = "y",
  coords = coords
)

chain <- dagify(
  q ~ x,
  y ~ q,
  exposure = "x",
  outcome = "y",
  coords = coords
)

collider <- dagify(
  q ~ x + y,
  exposure = "x",
  outcome = "y",
  coords = coords
)

dag_flows <- map(list(fork = fork, chain = chain, collider = collider), tidy_dagitty) |> 
  map("data") |> 
  list_rbind(names_to = "dag") |> 
  mutate(dag = factor(dag, levels = c("fork", "chain", "collider")))

dag_flows |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges(edge_width = 1) + 
  geom_dag_point() + 
  geom_dag_text() + 
  facet_wrap(~ dag) +
  expand_plot(
    expand_x = expansion(c(0.2, 0.2)),
    expand_y = expansion(c(0.2, 0.2))
  ) +
  theme_dag()
```

Forks represent a common cause of two variables. Here, we're saying that `q` causes both `x` and `y`. This is the traditional definition of a confounder. They're called forks because the arrows from `x` to `y` are in different directions. Chains, on the other hand, represent a series of arrows going in the same direction. Here `q` is called a *mediator*: it is along the causal path from `x` to `y`. In this diagram, the only path from `x` to `y` is the one mediated through `q`. Finally, a collider is a path where two arrowheads meet at a variable. Because causality always goes forward in time, this naturally means that the collider variable is caused by two other variables. Here, we're saying that `x` and `y` both cause `q`.

::: {.callout-tip}
## Are DAGs SEMs?
If you're familiar with structural equation models (SEMs), a modeling technique commonly used in psychology and other social science settings, you may notice some similarities between SEMs and DAGs. In fact, DAGs are a form of *non-parametric* SEM. SEMs estimate entire graphs using parametric assumptions. Causal DAGs, on the other hand, don't estimate anything; an arrow going from one variable to another says nothing about the strength or functional form of that relationship, only that we think it exists.
:::

One of the major benefits of DAGs is that they help us identify sources of bias and, often, provide clues in how to address them. However, talking about an unbiased effect estimate only makes sense when we have a specific causal question in mind. Since each arrow represents a cause, it's causality all the way down, so no individual arrow is inherently problematic. Here, we're interested in the effect of `x` on `y`. This question defines which paths we're interested in and which we're not. 

These three types of paths have different implications for the statistical relationship between `x` and `y`. If we only look at the correlation between the two variables under these assumptions:

1. In the fork, `x` and `y` will be associated, despite there being no arrow from `x` to `y`.
2. In the chain, `x` and `y` are related but only through `q`.
3. In the collider, `x` and `y` will *not* be related.

Paths that transmit association are called *open paths*. Paths that do not transmit association are called *closed paths*. Forks and chains are open while colliders are closed. 

So, should we adjust for `q`? That depends on the nature of the path. Forks are confounding paths. Because `q` causes both `x` and `y`, `x` and `y` will have a spurious association. They both contain information from `q`, their mutual cause, and that mutual causal relationship makes `x` and `y` associated statistically. Adjusting for `q` will *block* the bias from confounding and give us the true relationship between `x` and `y`. 

::: {.callout-tip}
## Adjustment
We can use a variety of techniques to account for a variable. We use the term "adjustment" to generally refer to any technique that removes the effect of variables we're not interested in.
:::

For chains, whether or not we adjusting for mediators depends on the research question. Here, adjusting for `q` would result in a null estimate of the effect of `x` on `y`. Because the only effect of `x` on `y` is via `q`, no other effect remains. The effect of `x` on `y` mediated by `q` is called the *indirect* effect, while the effect of `x` on `y` directly is called the *direct* effect. If we're only interested in the direct effect, controlling for `q` might be what we want. If we want to know about both effects, we shouldn't try to adjust for `q`. We'll learn more estimating different mediation effects in @sec-mediation.

Colliders are different. In this diagram, `x` and `y` are *not* associated, but both cause `q`. Adjusting for `q` has the opposite effect than with confounding: it *opens* a biasing pathway. Sometimes, people draw the path opened up by conditioning on a collider connecting `x` and `y`. 

Since `x` and `y` happen before `q`, `q` can't have an impact on them. If we break down the two time points in @fig-collider-time, at time point 1, `q` hasn't happened yet, and `x` and `y` are unrelated. At time point 2, `q` happens due to `x` and `y`. But causality only goes forward in time. `q` happening can't change the fact that `x` and `y` happened independently at time point 1.

```{r}
#| echo: false
#| label: fig-collider-time
collider_t <- collider |> 
  tidy_dagitty() |> 
  mutate(time = "time point 1", direction = NA, to = NA) |> 
  filter(name != "q")

t2 <- collider |> 
 tidy_dagitty() |> 
 mutate(time = "time point 2") |> 
  pull_dag_data()

collider_t$data <- bind_rows(collider_t$data, t2)

collider_t |> 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges(edge_width = 1) + 
  geom_dag_point() + 
  geom_dag_text() + 
  facet_wrap(~ time) + 
  theme_dag()
```

Causality only goes forward. Association, however, is time-agnostic. It's just an observation about the numerical relationships between variables. When we control for `q`, we introduce an association between `x` and `y`. How can this be? Consider a case where `x` and `y` are the only causes of `q`, and all three variables are binary. When either `x` or `y` equals 1, then `m` happens. If we know `q = 1` and `x = 0` then logically it must be that `y = 1`. Thus, knowing about `q` gives us information about `y` via `x`. 

Visually, we can see this happen when `x` and `y` are continuous and `q` is binary. In @fig-collider-scatter, when we don't include `q`, we find there is no relationship between `x` and `y`. That's the correct result. However, when we include `q`, we can information about both `x` and `y`, and they appear correlated: across levels of `x`, those with `q = 0` have lower levels of `y`. Association seemingly flows back in time. Of course, that can't happen from a causal perspective, so controlling for `q` is the wrong thing to do. We end up with a biased effect of `x` on `y`.

```{r}
#| echo: false
#| label: fig-collider-scatter
#| message: false
set.seed(123)
library(patchwork)
n <- 500
x <- rnorm(n)
y <- rnorm(n)
linear_pred <- 2*x + 3*y + rnorm(n)
prob <- 1 / (1 + exp(-linear_pred))

q <- rbinom(n, size = 1, prob = prob)

collider_data <- tibble(x, y, q = as.factor(q))

p1 <- collider_data |> 
  ggplot(aes(x, y)) + 
  geom_point(alpha = .35) + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  facet_wrap(~"not adjusting for `q`")

p2 <- collider_data |> 
  ggplot(aes(x, y, color = q)) + 
  geom_point(alpha = .35) + 
  geom_smooth(method = "lm", se = FALSE) + 
  facet_wrap(~"adjusting for `q`")

p1 + p2
```

::: {.callout-tip}
## Exchangability revisited
We commonly refer exchangability as the assumption of no confounding. Actually, this isn't quite right. It's the assumption of no open non-causal paths. Many times, these are confounding pathways. However, paths can also be opened by conditioning on a collider. Even though these aren't colliders, it creates non-exchangability between the two groups: they are different in a way that matters to the exposure and outcome.
:::



- basic terms
- types of paths
- what to adjust for
- popout: intuition for colliders
- popout: exchangability revisited
- popout: what about interaction?

## DAGs in R

- building
- popout: coordinates. show manual. mention dagitty.net. mention `time_ordered`.
- paths
- adjustment sets
- with ggplot

## Common Structures of Bias

- advanced forms of confounding, e.g. L happens after X
- Selection bias, M-Bias, Butterfly bias. L2FU later.
- instrumental variables, precision/competing exposure variables

## Recommendations in building DAGs

### Iterate early and often

- Ideally before when designing your research, at least before analyzing data (avoid overfitting)

### Consider your question

- estimand
- population and context

### Order nodes by time

- Time ordering algorithm
- Feedback loops: global warming and A/C use

### Consider the whole data collection process

- race/shooting
- healthy worker bias

### Include variables you don't have

- Examples where you can and can't adjust depending

### Saturate your DAG by default

### Include instruments and competing exposures

### Focus on the causal structure, then consider measurement bias

### Be accurate, but focus on clarity

### Pick adjustment sets most likely to be succesful

- measurement error, certainty
- use the path with the most observed variables

### Use robustness checks

- Negative controls/Falsification end-points, dag-data consitency, alternate adjustment sets

## Causal Inference is not (just) a statistical problem {#sec-quartets}

### Causal and Predictive Models, Revisited {#sec-causal-pred-revisit}

- Probably too long, but if possible, condense to a popout
- DAGs showing examples where prediction can lean on measured confounders, colliders. It's the amount of information a variable brings, not whether the coeffecient is unbiased affect of variable on outcome.
- Not practical to fit a prediction model with future variable
- Table 2 Bias examples. Unmeasure confounding of Z-Y relationship. Mediation example.

